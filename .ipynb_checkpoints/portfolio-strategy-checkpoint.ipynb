{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c337505d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/S000006408.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myfinance\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01myf\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbeta_values\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     11\u001b[0m days_in_quarter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m63\u001b[39m\n",
      "File \u001b[0;32m~/Erdos/seeking-thunder/beta_values.py:51\u001b[0m\n\u001b[1;32m     48\u001b[0m ticker \u001b[38;5;241m=\u001b[39m series_to_ticker_mapping[series]\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Read the CSV\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     52\u001b[0m df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTickers\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     53\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[:,:\u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/S000006408.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import json\n",
    "import difflib\n",
    "import yfinance as yf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from beta_values import *\n",
    "\n",
    "days_in_quarter = 63"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271499c0",
   "metadata": {},
   "source": [
    "We describe our strategy in four parts:\n",
    "\n",
    "(1)  **When to invest?**\n",
    "\n",
    "The idea of our strategy is to invest after a day during which the ETF volume is statistically large and the ETF volume is negative.  We compute these days in `high_volume_neg_return_days` below. The reason we focus on these days is that, on these days, the correlations among the stocks in the ETF is abnormally high.  These large selloffs are often a response to a large macro shock that affects many of the stocks within the ETF.  The idea of our strategy is based on the theory that, when these large ETF selloffs happen, some stocks in the ETF are \"dragged down\" along with the ETF even though, from a fundamental perspective, they should be less exposed to the macro shock that drove the selloff.\n",
    "\n",
    "(2)  **Which stocks to invest in?**\n",
    "\n",
    "We want to invest in stocks we label *outsider* stocks.  Roughly speaking, these are stocks within an ETF that respond less strongly to the movements of the ETF.  The intuition is that these outsider stocks are generally less likely to be affected by the macro developments that affect the ETF, and so these are the stocks whose prices are most likely to be distorted during large selloffs of the ETF.\n",
    "\n",
    "How do we precisely measure this notion of *outsider* stock?  \n",
    "\n",
    "To measure the stocks that respond less strongly to the movements of the ETF, we compute the coefficient $\\beta$ of the exponentially weighted linear regression of the returns of the stock against the returns of the ETF.  (The reason we introduce this exponential weighting is that we want to weight more recent returns more heavily than less recent ones.  We used the same weighting approach in our calculation of the ETF volume spikes above.)  \n",
    "\n",
    "This coefficient $\\beta$ is called the *ETF beta* of the stock.  Note that the ETF beta changes from day to day, since we must update our regression model each day based on the new day's returns.  In `betas_each_day` (constructed in the notebook `beta_values.py`) we compile, for each ETF, a dataframe that contains these ETF betas for each stock and each day during the time period we are studying.\n",
    "\n",
    "Once we have computed these ETF betas, we define the ETF's outsider stocks on a given day as the stocks whose ETF betas that day fall in the bottom 10 percent. In `outsider_stocks`, we assign a value of `True` to a stock on a given day if that stock is an outsider to its ETF on that day.\n",
    "\n",
    "(3)  **How do we measure the performance of our choice of stock against the ETF?**\n",
    "\n",
    "Once we choose a day $D$ and a stock $S$ within an ETF, we must determine the following: if we invested in the stock $S$ at the *close* of day $D$ and held it for 40 days, how does the stock's return $r_S$ compare with the return $r_{ETF}$ of the ETF over that same period?  Since we want to measure our portfolio's alpha compared with the ETF, we leverage our portfolio with the ETF beta value $\\beta_S$ of the stock on day $D$.  Therefore, we define the alpha of the stock's 40-day return versus the ETF as $r_S/\\beta_S - r_{ETF}$.  These alphas are computed in `stock_40_day_alpha`.\n",
    "\n",
    "We emphasize that `stock_40_day_alpha` does not specify *which* stocks to invest in, or *when* to invest in these stocks.  It tells us that *if* we invest in this stock following this trading day, what is the associated alpha value: i.e., to what degree does this stock's leveraged 40-day return outperform that of the ETF.\n",
    "\n",
    "(4)  **Putting it all together.**\n",
    "\n",
    "Now that we have determined which stocks to invest in and when, and how to measure the performance of our investment choice, we can combine these steps to evaluate the performance of our overall strategy.  In `portfolio_alphas`, we multiply each pair of dataframes in `outsider_stocks` and `stock_40_day_alpha` to obtain the alphas associated to each of the investments dictated by our strategy.  In `portfolio_average_alphas`, we average over each of these dataframes to get, for each ETF, the average value of alpha for our investments in that ETF's stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "edebbac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of ETFs we consider in this work.\n",
    "\n",
    "etf_tickers = {\n",
    "    'XLY',\n",
    "    'XLP',\n",
    "    'XLE',\n",
    "    'XLF',\n",
    "    'XLV',\n",
    "    'XLI',\n",
    "    'XLB',\n",
    "    'XLK',\n",
    "    'XLU',\n",
    "    'XLRE',\n",
    "    'XLC'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f663fa0",
   "metadata": {},
   "source": [
    "(1)  **When to invest?**  We calculate the days with statistically high ETF volume and negative ETF returns.  We store this information in `high_volume_neg_return_days`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99acc585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code retrieves the ETF beta values of the stocks in each ETF compiled in `beta_values.py`.\n",
    "# Note that the ETF beta on a given day depends only on returns from previous days (not including that day).\n",
    "\n",
    "betas_per_day = {}\n",
    "\n",
    "for etf in etf_tickers:\n",
    "    df = pd.read_csv(f\"data/{etf}_betas_per_day.csv\")\n",
    "    df.set_index(df.columns[0],inplace=True)\n",
    "    df.index.name = 'Date'\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    betas_per_day[etf] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db84634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate when the volume of a stock or ETF is \"statistically large\".\n",
    "\n",
    "def exp_weighted_z_score(data,halflife):\n",
    "    return (data - data.ewm(halflife=halflife).mean().shift(1)) / data.ewm(halflife=halflife).std().shift(1) \n",
    "\n",
    "def high_volume_negative_return(ticker, threshold=3, halflife=days_in_quarter):\n",
    "  \n",
    "    negative_returns = returns(ticker) < 0\n",
    "    \n",
    "    vol = yf.Ticker(ticker).history(period='max').Volume.tz_localize(None)    \n",
    "    vol_z_scores = exp_weighted_z_score(vol,halflife=halflife)\n",
    "    \n",
    "    high_vol_neg_return = (vol_z_scores >= threshold) * negative_returns\n",
    "    \n",
    "    return high_vol_neg_return\n",
    "\n",
    "high_volume_neg_return_days = {}\n",
    "\n",
    "for etf in etf_tickers:\n",
    "    high_volume_neg_return_days[etf] = high_volume_negative_return(etf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1fc721",
   "metadata": {},
   "source": [
    "(2)  **Which stocks to invest in?**  We calculate each day's \"outsider stocks\" in each ETF.  (Note that the outsider stocks for a given day is determined by information from *before* that day.)  We store this information in `outsider_stocks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33e1fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the outsider stocks in an ETF on a given day as those whose ETF betas that day \n",
    "# are in the bottom 10 percent.\n",
    "\n",
    "outsider_stocks = {}\n",
    "\n",
    "def is_in_lower_quantile(row,level=0.1):\n",
    "    threshold = row.quantile(level)\n",
    "    return row <= threshold\n",
    "\n",
    "for etf in etf_tickers:\n",
    "    outsider_stocks[etf] = betas_per_day[etf].apply(is_in_lower_quantile, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb71e1b",
   "metadata": {},
   "source": [
    "(3)  **How do we measure the performance of our choice of stock against the ETF?**  \n",
    "\n",
    "We compute the alpha from investing in a stock at the close of the given day nad holding for 40 days, leveraged against the ETF, compared to investing in the ETF for those 40 days.  We store the results in `stock_40_day_alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d161d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "holding_period = 40 \n",
    "\n",
    "# One can test other values to see what happens when you instead hold the stock for a different \n",
    "# number of days.  We are using the time horizon of 40 days based on the testing in Lynch et al.'s work.\n",
    "# In future analysis, it would be interesting to explore varying this value based on developments \n",
    "# in the sector and in the overall market. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8b314dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_log_returns = {}\n",
    "stock_40_day_returns = {}\n",
    "stock_40_day_alpha = {}\n",
    "\n",
    "for etf in etf_tickers:      \n",
    "\n",
    "    # Compile tickers of both the ETF and all stocks in the ETF's holdings from the time period being considered\n",
    "    tickers = []\n",
    "    tickers.append(etf)\n",
    "    stocks = holdings_per_day[etf].columns\n",
    "    tickers.extend(stocks.values.tolist())\n",
    "\n",
    "    # Compute the log returns of each of these tickers during the time period\n",
    "    \n",
    "    start_date = holdings_per_day[etf].index[0]\n",
    "    end_date = holdings_per_day[etf].index[-1]\n",
    "    \n",
    "    etf_stock_log_returns = pd.DataFrame({ticker: log_returns(ticker,start_date=start_date,end_date=end_date) for ticker in tickers})\n",
    "    \n",
    "    # Compute, for each day, the returns from buying the stock AFTER that day and holding for 40 days\n",
    "    \n",
    "    etf_stock_40_day_returns = np.exp(sum(etf_stock_log_returns.shift(-i) for i in range(1,1 + holding_period))) - 1\n",
    "    \n",
    "    # Compute the difference between the stock's 40-day return that we just computed with the corresponding 40-day return of the ETF, leveraging by the stock's ETF beta\n",
    "    \n",
    "    df = etf_stock_40_day_returns[stocks].copy()\n",
    "\n",
    "    for stock in stocks:\n",
    "        df[stock] = (df[stock] / betas_per_day[etf][stock]) - etf_stock_40_day_returns[etf]\n",
    "        \n",
    "    df = df.dropna(how='all')\n",
    "       \n",
    "    etf_stock_40_day_alpha = df\n",
    "       \n",
    "    # Add all the information just computed to different dictionaries, indexing that information by the ETF ticker\n",
    "    \n",
    "    stock_log_returns[etf] = etf_stock_log_returns\n",
    "    stock_40_day_returns[etf] = etf_stock_40_day_returns\n",
    "    stock_40_day_alpha[etf] = etf_stock_40_day_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40296068",
   "metadata": {},
   "source": [
    "(4)  **Putting it all together.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "86e2de52",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = {}\n",
    "alphas_mean = {}\n",
    "\n",
    "for etf in etf_tickers:\n",
    "    \n",
    "    index = stock_40_day_alpha[etf].index \n",
    "    \n",
    "    outsiders = outsider_stocks[etf].shift(-1).reindex(index)\n",
    "    high_vol_neg_return = high_volume_neg_return_days[etf].reindex(index)\n",
    "    \n",
    "    outsiders = outsiders.mul(high_vol_neg_return, axis=0).replace(False, np.nan)\n",
    "    \n",
    "    alphas[etf] = outsiders * stock_40_day_alpha[etf]\n",
    "    \n",
    "    alphas[etf] = alphas[etf].dropna(how='all')\n",
    "    \n",
    "    alphas_mean[etf] = alphas[etf].mean(axis=1).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
